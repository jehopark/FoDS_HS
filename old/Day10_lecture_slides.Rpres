[Foundations of Data Science with Capstone at SMU, Summer 2021]
Day 10: Functions, Model Basics, and Capstone
========================================================
author: Dr. Jeho Park
date: 
autosize: true

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(eval = FALSE)
library(tidyverse)

library(modelr)
options(na.action = na.warn)
```

DAY9 and LAB9 Review
========================================================

Day9 Quiz

Lab9: 19.4.4 #3   
https://jrnold.github.io/r4ds-exercise-solutions/functions.html#exercise-19.4.3

Any questions?


Today's Topic
============
- Iteration
- Modeling Data
- Data Science Capstone Project

Iteration
==============
See examples at https://r4ds.had.co.nz/iteration.html#for-loops

1. Prepare output first   

2. Use sequence: `i in seq_along(df)`   

3. Body should contain proper indexing
 

```{r}
output <- vector("double", ncol(df))  # 1. output
for (i in seq_along(df)) {            # 2. sequence
  output[[i]] <- median(df[[i]])      # 3. body
}
 
```

Modeling Real World 
================
![Model2](images/architect-model.jpeg)

Modeling Data
============
![Model](images/data-science-model.png)

You’ve tidied and cleaned your data, done some visualizations to see what’s going on,and think you have a pattern. What next? 

One of the key pieces of data science is modeling your data. A model is an simplification of the real world that allows us to predict outcomes. 

>> The goal of a model is not to uncover truth, but to discover a simple approximation that is still useful.


Model Basics
============
__A Simple Model__

In the `modelr` package, there is a simulated dataset `sim1`.

```{r echo=TRUE}
library(modelr) 
ggplot(sim1, aes(x, y)) + 
  geom_point()
```


In this case, the relationship looks linear, i.e. `y = a_0 + a_1 * x`

When we consider the data values, (x[1], y[1]), (x[2], y[2]), ..., the linear equation above should be represented by:

y_i = a_0 + a_1 * x_i + e_i , 

where (x_i, y_i) is the i_th observation, and e_i is called error.

The question here is __"What should a_0 and a_1 be?"__ to model the data?

A Simple Approach I
===============
__A line connecting the middle `y` value when `x = 1`, and the middle `y` value when `x = 10`__

```{r}
sim1 %>%
  filter(x == 1 | x == 10) %>%
  arrange(y)
```

We have P1 = (1, 4.2) and P2 = (10, 23.3).

The slope is (23.3 - 4.2)/(10 - 1)
```{r}
m <- (23.3 - 4.2)/(10 - 1)
```

And the y-intercept is
```{r}
y_intercept <- 4.2 - (m*1) # using P1 = (1, 4.2)
```

A Simple Approach I (cont)
===============

So, the line that may predict our data is 

```{r echo=TRUE}
sim1 %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_abline(aes(intercept = y_intercept, slope = m),col = "blue", lwd = 1)
```

Notice here we used `geom_abline()` function.

A Simple Approach I (cont)
===============
__What about the error?___

![Error](images/simple-error.png)

The best ideal fit will have the error = 0. But it is impossible for the real world data. So, we are interested in finding a line (linear model) that has the smallest possible error.

>> In a model, the choice of parameters than minimizes __the sum of the squares of the errors__ is called the __least squares__ fit.

A Simple Approach I: Error!
========
To find these errors, let’s make a function that is our model that takes a tibble or data frame with variable x, and returns a prediction given the slope and intercept of a linear model.

```{r}
model1 <- function(a, data) {
  return(a[1] + a[2]*data$x)
}
model1(c(2.07, 2.13), sim1) # Predicted values
```

Now let’s measure the sum of the squares of the error, when the response values are in variable y.
```{r}
sum_square_error <- function(mod, data) {
  diff <- data$y -model1(mod, data)
  return(sum(diff^2))
}
sum_square_error(c(2.07, 2.13), sim1)
```

Let's change the model parameters and see if the error changes:
```{r}
sum_square_error(c(2.1, 2.13), sim1)
```

A Simple Approach I: Minimize the Error (using optim)
========
By playing around with the intercept and slope, we can make this smaller and smaller. In fact,R has a built in function to do exactly that. It is called `optim`, and it works using a method called Newton-Raphson.

```{r}
optimal_coef <- optim(c(2.07, 2.13), sum_square_error, data = sim1)

optimal_coef$par
```

```{r}
sim1 %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_abline(aes(intercept = optimal_coef$par[1], slope = optimal_coef$par[2]),col = "red", lwd = 1)
```

Linear Model in R
===============
A linear model has the general form `y = a_1 + a_2 * x_1 + a_3 * x_2 + ... + a_n * x_(n - 1)`. 

R has a tool specifically designed for fitting linear models called `lm()`. 

`lm()` has a special way to specify the model family: formulas. 

Formulas look like `y ~ x`, which lm() will translate to a function like `y = a_1 + a_2 * x`. 

The ∼ character is used to designate the model. The variable to be predicted goes on the left of ∼, and the variables used to predict go on the right. So for this model, we can use: 

```{r}
sim1_mod <- lm(y ~ x, data = sim1)
coef_lm <- coef(sim1_mod)
```

Note: Currently, the only type of error minimization that R can do is __least squares__.

ggplot2 has a simple way to visualize the lm line:

```{r}
sim1 %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", col = "green", se = FALSE)
```

Data Science Capstone Project
=================
각 팀은 다음과 같은 task를 진행하여야 합니다.   
(1) 데이터 프로젝트 정하기  
(2) 데이터 모으기  
(3) 데이터 정리하기   
(4) 데이터 시각화와 데이터 분석을 통하여 결과 도출하기  
(5) GitHub에 리포트를 올리거나 다른 서비스로 Dashboard 만들기  
(6) 결과를 10분 정도의 프리젠테이션으로 발표하기  

- 모든 task는 log를 하여 매일 강사에게 보냅니다. Work log는 Google Sheet (or Excel)을 사용해도 무방하고 Google Doc (Word)를 사용해도 됩니다.

- Daily Sync: 매일 10분~20분 각 팀과 강사가 온라인으로 만나서 그 전 날에 이뤄진 task의 진행 상황을 체크하고 앞으로의 진행 방향을 정합니다.  

- Daily Sync를 위해 각 팀은 다음 중에서 실시간으로 만날 수 있는 시간을 정해서 강사에게 이메일로 알립니다.
  - 9:30 AM, 10:00 AM, 10:30 AM, 11:00 AM, 11:30 AM, 12:00 PM, 1:30 PM, 2:00 PM, 2:30 PM, 3:00 PM

- 만약 온라인으로 실시간 만날 수 없을 경우 다른 communication 방법 (이메일, 카톡등) 을 사용하여 강사에게 보고하고 진행 방향을 정합니다.



Lab10
=====
- 21.2.1: #1
- 23.2.1: #1

```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)

sim1a %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", col = "red", se = FALSE)

```

